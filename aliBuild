#!/usr/bin/env python
import os, sys, shutil, subprocess, logging, time, re, yaml, hashlib, argparse
from commands import getstatusoutput
from os.path import basename, dirname, abspath, exists, realpath, join
from os import makedirs, unlink, readlink
from glob import glob
from logging import debug,error
from datetime import datetime

def format(s, **kwds):
  return s % kwds

def star():
  return basename(sys.argv[0]).replace("Build", "")

def gzip():
  return getstatusoutput("which pigz")[0] and "gzip" or pigz

def execute(command, printer=debug):
  popen = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE)
  lines_iterator = iter(popen.stdout.readline, "")
  for line in lines_iterator:
    debug(line.strip("\n"))  # yield line
  output = popen.communicate()[0]
  debug(output)
  exitCode = popen.returncode
  return exitCode

def dieOnError(error, msg):
  if error:
    print msg
    sys.exit(1)

def updateReferenceRepos(referenceSources, p, spec):
  # Update source reference area, if possible.
  # If the area is already there and cannot be written, assume it maintained
  # by someone else.
  #
  # If the area can be created, clone a bare repository with the sources.
  debug("Updating references.")
  referenceRepo = "%s/%s" % (abspath(referenceSources), p.lower())
  if os.access(dirname(referenceSources), os.W_OK):
    getstatusoutput("mkdir -p %s" % referenceSources)
  writeableReference = os.access(referenceSources, os.W_OK)
  if not writeableReference and exists(referenceRepo):
    debug("Using %s as reference for %s." % (referenceRepo, p))
    spec["reference"] = referenceRepo
    return
  if not writeableReference:
    debug("Cannot create reference for %s in specified folder.", p)
    return

  err, out = getstatusoutput("mkdir -p %s" % abspath(referenceSources))
  if not "source" in spec:
    return
  if not exists(referenceRepo):
    cmd = ["git", "clone", "--bare", spec["source"], referenceRepo]
    debug(" ".join(cmd))
    err = execute(" ".join(cmd))
  else:
    err = execute(format("cd %(referenceRepo)s && "
                         "git fetch --tags %(source)s 2>&1 && "
                         "git fetch %(source)s 2>&1",
                         referenceRepo=referenceRepo,
                         source=spec["source"]))
  if err:
    print "Error while updating reference repos %s." % spec["source"]
    sys.exit(1)
  spec["reference"] = referenceRepo

def getDirectoryHash(d):
  err, out = getstatusoutput("GIT_DIR=%s/.git git show-ref HEAD | cut -f1 -d\ " % d)
  if err:
    print "Impossible to find reference for %s " % d
    sys.exit(0)
  return out

if __name__ == "__main__":
  parser = argparse.ArgumentParser()
  parser.add_argument("action")
  parser.add_argument("pkgname")
  parser.add_argument("--config-dir", "-c", dest="configDir", default="%sdist" % star())
  parser.add_argument("--devel", dest="devel", default=[])
  parser.add_argument("--work-dir", "-w", dest="workDir", default="sw")
  parser.add_argument("--architecture", "-a", dest="architecture", required=True)
  parser.add_argument("--jobs", "-j", dest="jobs", default=1)
  parser.add_argument("--reference-sources", dest="referenceSources", default="sw/MIRROR")
  parser.add_argument("--remote-store", dest="remoteStore", default="",
                      help="Where to find packages already built for reuse."
                           "Use ssh:// in front for remote store. End with ::rw if you want to upload.")
  parser.add_argument("--write-store", dest="writeStore", default="",
                      help="Where to upload the built packages for reuse."
                           "Use ssh:// in front for remote store.")
  parser.add_argument("--debug", "-d", dest="debug", action="store_true", default=False)
  args = parser.parse_args()

  logger = logging.getLogger()
  logger_handler = logging.StreamHandler()
  logger.addHandler(logger_handler)

  if args.debug:
    logger.setLevel(logging.DEBUG)
    logger_handler.setFormatter(logging.Formatter('%(levelname)s: %(message)s'))
  else:
    logger.setLevel(logging.INFO)

  rsyncOptions = ""
  if args.remoteStore.startswith("ssh://"):
    args.remoteStore = args.remoteStore[6:]
  if args.writeStore.startswith("ssh://"):
    args.writeStore = args.writeStore[6:]
  if args.remoteStore.endswith("::rw") and args.writeStore:
    print "You cannot specify ::rw and --write-store at the same time"
    sys.exit(1)
  if args.remoteStore.endswith("::rw"):
    args.remoteStore = args.remoteStore[0:-4]
    args.writeStore = args.remoteStore

  if args.devel:
    args.devel = args.devel.split(",")
    print "Write store disabled since --devel option passed."
    args.writeStore = ""

  # Setup build environment.
  if not args.action == "build":
    parser.error("Action %s unsupported" % args.action)

  packages = [args.pkgname]
  specs = {}
  buildOrder = []
  workDir = abspath(args.workDir)
  specDir = "%s/SPECS" % workDir
  if not exists(specDir):
    makedirs(specDir)

  debug(format("Using %(star)sBuild from "
               "%(star)sbuild@%(toolHash)s recipes "
               "in %(star)sdist@%(distHash)s",
               star=star(),
               toolHash=getDirectoryHash(dirname(__file__)),
               distHash=getDirectoryHash(args.configDir)))

  while packages:
    p = packages.pop(0)
    if p in specs:
      buildOrder.remove(p)
      buildOrder.insert(0, p)
      continue
    try:
      d = open("%s/%s.sh" % (args.configDir, p.lower())).read()
    except IOError,e:
      print e
      sys.exit(1)
    header, recipe = d.split("---", 1)
    spec = yaml.safe_load(header)
    # Check that version is a string
    if not isinstance(spec["version"], basestring):
      print "In recipe \"%s\": version must be a string" % p
      sys.exit(1)
    spec["tag"] = spec.get("tag", spec["version"])
    spec["version"] = spec["version"].replace("/", "_")
    spec["recipe"] = recipe.strip("\n")
    specs[spec["package"]] = spec
    buildOrder.insert(0, spec["package"])
    realRequires = []
    for require in spec.get("requires", []):
      matcher = ".*"
      if ":" in require:
        require, matcher = require.split(":", 1)
      if not re.match(matcher, args.architecture):
        debug("Not appending %s as a requirement" % require)
        continue
      debug("appending %s as a requirement" % require)
      packages.append(require)
      realRequires.append(require)
    spec["requires"] = realRequires
      
    open("%s/SPECS/%s.sh" % (workDir, spec["package"]), "w").write(spec["recipe"])

  # Resolve the tag to the actual commit ref, so that
  for p in buildOrder:
    spec = specs[p]
    spec["commit_hash"] = "0"
    if "source" in spec:
      # Replace source with local one if we are in development mode.
      if spec["package"] in args.devel:
        spec["source"] = join(os.getcwd(), spec["package"])

      cmd = format("git ls-remote --heads %(source)s",
                   source = spec["source"])
      err, out = getstatusoutput(cmd)
      if err:
        print "Unable to fetch from %s" % spec["source"]
        sys.exit(1)
      # by default we assume tag is a commit hash
      spec["commit_hash"] = spec["tag"]
      for l in out.split("\n"):
        if l.endswith("refs/heads/%s" % spec["tag"]):
          spec["commit_hash"] = l.split("\t", 1)[0]
          # We are in development mode, we need to rebuild if the commit hash
          # is different and if there are extra changes on to.
          if spec["package"] in args.devel:
            cmd = format("cd %(source)s ; git diff -r HEAD | sha1sum",
                         source = spec["source"])
            debug(cmd)
            err, out = getstatusoutput(cmd)
            debug(out)
            dieOnError(err, "Unable to detect source code changes.")
            spec["devel_hash"] = spec["commit_hash"] + out
            spec["commit_hash"] = "0"
          break

  # Decide what is the main package we are building and at what commit.
  #
  # We emit an event for the main package, when encountered, so that we can use
  # it to index builds of the same hash on different architectures. We also
  # make sure add the main package and it's hash to the debug log, so that we
  # can always extract it from it.
  # If one of the special packages is in the list of packages to be built,
  # we use it as main package, rather than the last one.
  mainPackage = None
  mainHash = None
  for p in buildOrder:
    spec = specs[p]
    mainPackage = p
    mainHash = spec["commit_hash"]
    if p.lower() in ["aliroot", "aliphysics", "o2"]:
      break
  debug("Main package is %s@%s" % (mainPackage, mainHash))
  if args.debug:
    logger_handler.setFormatter(
        logging.Formatter("%%(levelname)s:%s:%s: %%(message)s" %
                          (mainPackage, mainHash[0:8])))

  # Now that we have the main package set, we can print out Useful information
  # which we will be able to associate with this build.
  for p in buildOrder:
    spec = specs[p]
    if "source" in spec:
      debug("Commit hash for %s@%s is %s" % (spec["source"], spec["tag"], spec["commit_hash"]))

  # Expand variables found in the version.  You can define things like date
  # with %(year)s %(month)s %(day)s %(hour)s and you can refer to some of the
  # other properties like %(tag)s %(commit_hash)s
  now = datetime.now()
  for p in buildOrder:
    spec = specs[p]
    spec["version"] = format(spec["version"],
                             commit_hash=spec["commit_hash"],
                             short_hash=spec["commit_hash"][0:10],
                             tag=spec["tag"],
                             year=str(now.year),
                             month=str(now.month),
                             day=str(now.day),
                             hour=str(now.hour))

  # Calculate the hashes. We do this in build order so that we can guarantee
  # that the hashes of the dependencies are calculated first.  Also notice that
  # if the commit hash is a real hash, and not a tag, we can safely assume
  # that's unique, and therefore we can avoid putting the repository or the
  # name of the branch in the hash.
  debug("Calculating hashes.")
  for p in buildOrder:
    spec = specs[p]
    h = hashlib.sha1()
    for x in ["recipe", "version", "package", "commit_hash",
              "env", "append_path", "prepend_path"]:
      h.update(str(spec.get(x, "none")))
    if spec["commit_hash"] == spec.get("tag", "0"):
      h.update(spec.get("source", "none"))
      if "source" in spec:
        h.update(spec["tag"])
    for dep in spec.get("requires", []):
      h.update(specs[dep]["hash"])
    if bool(spec.get("force_rebuild", False)):
      h.update(str(time.time()))
    if spec["package"] in args.devel and "incremental_recipe" in spec:
      h.update(spec["incremental_recipe"])
      incremental_hash = hashlib.sha1(spec["incremental_recipe"]).hexdigest()
      spec["incremental_hash"] = "INCREMENTAL_BUILD_HASH=%s" % incremental_hash
    elif p in args.devel:
      h.update(spec.get("devel_hash"))
    spec["hash"] = h.hexdigest()
    debug("Hash for recipe %s is %s" % (p, spec["hash"]))

  # This adds to the spec where it should find, locally or remotely the
  # various tarballs and links.
  for p in buildOrder:
    spec = specs[p]
    pkgSpec = {
      "repo": workDir,
      "package": spec["package"],
      "version": spec["version"],
      "hash": spec["hash"],
      "prefix": spec["hash"][0:2],
      "architecture": args.architecture
    }
    tarSpecs = [
      ("storePath", "TARS/%(architecture)s/store/%(prefix)s/%(hash)s"),
      ("linksPath", "TARS/%(architecture)s/%(package)s"),
      ("tarballHashDir", "%(repo)s/TARS/%(architecture)s/store/%(prefix)s/%(hash)s"),
      ("tarballLinkDir", "%(repo)s/TARS/%(architecture)s/%(package)s")
    ]
    spec.update(dict([(x, format(y, **pkgSpec)) for (x, y) in tarSpecs]))

  for p in buildOrder:
    spec = specs[p]
    todo = [p]
    spec["full_requires"] = []
    while todo:
      requires = specs[todo.pop(0)].get("requires", [])
      spec["full_requires"] += requires
      todo += requires
    spec["full_requires"] = set(spec["full_requires"])

  # We now iterate on all the packages, making sure we build correctly every
  # single one of them. This is done this way so that the second time we run we
  # can check if the build was consistent and if it is, we bail out.
  packageIterations = 0
  while buildOrder:
    packageIterations += 1
    if packageIterations > 20:
      print "Too many attempts at building %s. Something wrong with the repository?"
      exit(1)
    p = buildOrder[0]
    spec = specs[p]
    # Since we can execute this multiple times for a given package, in order to
    # ensure consistency, we need to reset things and make them pristine.
    spec.pop("revision", None)

    debug("Updating from tarballs")
    # If we arrived here it really means we have a tarball which was created
    # using the same recipe. We will use it as a cache for the build. This means
    # that while we will still perform the build process, rather than
    # executing the build itself we will:
    #
    # - Unpack it in a temporary place.
    # - Invoke the relocation specifying the correct work_dir and the
    #   correct path which should have been used.
    # - Move the version directory to its final destination, including the
    #   correct revision.
    # - Repack it and put it in the store with the
    #
    # this will result in a new package which has the same binary contents of
    # the old one but where the relocation will work for the new dictory. Here
    # we simply store the fact that we can reuse the contents of cachedTarball.
    if args.remoteStore:
      debug("Updating remote store for package %s@%s" % (p, spec["hash"]))
      cmd = format("mkdir -p %(tarballHashDir)s\n"
                   "rsync -av %(rsyncOptions)s %(remoteStore)s/%(storePath)s/ %(tarballHashDir)s/ || true\n"
                   "rsync -av --delete %(rsyncOptions)s %(remoteStore)s/%(linksPath)s/ %(tarballLinkDir)s/ || true\n",
                   rsyncOptions=rsyncOptions,
                   remoteStore=args.remoteStore,
                   storePath=spec["storePath"],
                   linksPath=spec["linksPath"],
                   tarballHashDir=spec["tarballHashDir"],
                   tarballLinkDir=spec["tarballLinkDir"])
      err = execute(cmd)
      dieOnError(err, "Unable to update from specified store.")
    # Decide how it should be called, based on the hash and what is already
    # available.
    debug("Checking for packages already built.")
    linksGlob = format("%(w)s/TARS/%(a)s/%(p)s/%(p)s-%(v)s-*.%(a)s.tar.gz",
                       w=workDir,
                       a=args.architecture,
                       p=spec["package"],
                       v=spec["version"])
    debug("Glob pattern used: %s" % linksGlob)
    packages = glob(linksGlob)
    # In case there is no installed software, revision is 1
    # If there is already an installed package:
    # - Remove it if we do not know its hash
    # - Use the latest number in the version, to decide its revision
    debug("Packages already built using this version\n%s" % "\n".join(packages))
    if not packages:
      spec["revision"] = 1
    busyRevisions = []
    for d in packages:
      realPath = readlink(d)
      matcher = format("../../%(a)s/store/[0-9a-f]{2}/([0-9a-f]*)/%(p)s-%(v)s-([0-9]*).%(a)s.tar.gz",
                       a=args.architecture,
                       p=spec["package"],
                       v=spec["version"])
      m = re.match(matcher, realPath)
      if not m:
        continue
      h, revision = m.groups()
      revision = int(revision)
      # If we have an hash match, we use the old revision for the package
      # and we do not need to build it.
      if h == spec["hash"]:
        spec["revision"] = revision
        if spec["package"] in args.devel and "incremental_recipe" in spec:
          spec["obsolete_tarball"] = d
        else:
          print "Package %s with hash %s is already found in %s. Not building." % (p, h, d)
        break
      else:
        busyRevisions.append(revision)

    if not "revision" in spec:
      spec["revision"] = min(set(range(1, max(busyRevisions)+2)) - set(busyRevisions))

    # Now that we have all the information about the package we want to build, let's
    # check if it wasn't built / unpacked already.
    hashFile = "%s/%s/%s/%s-%s/.build-hash" % (workDir, 
                                               args.architecture, 
                                               spec["package"],
                                               spec["version"],
                                               spec["revision"])
    try:
      fileHash = open(hashFile).read().strip("\n")
    except:
      fileHash = "0"
    if fileHash != spec["hash"]:
      if fileHash != "0":
        debug("Mismatch between local area and the one which I should build. Redoing.")
      shutil.rmtree(dirname(hashFile), True)
    else:
      # If we get here, we know we are in sync with whatever remote store.  We
      # can therefore create a directory which contains all the packages which
      # were used to compile this one.
      debug("Package %s was correctly compiled. Moving to next one." % spec["package"])
      # If using incremental builds, next time we execute the script we need to remove
      # the placeholders which avoid rebuilds.
      if spec["package"] in args.devel and "incremental_recipe" in spec:
        unlink(hashFile)
      if "obsolete_tarball" in spec:
        unlink(realpath(spec["obsolete_tarball"]))
        unlink(spec["obsolete_tarball"])
      distributionLinks = []
      target = format("TARS/%(a)s/dist/%(p)s/%(p)s-%(v)s-%(r)s",
                      a=args.architecture,
                      p=spec["package"],
                      v=spec["version"],
                      r=spec["revision"])
      shutil.rmtree(target, True)
      for x in [spec["package"]] + list(spec["full_requires"]):
        dep = specs[x]
        source = format("../../../../../TARS/%(a)s/store/%(sh)s/%(h)s/%(p)s-%(v)s-%(r)s.%(a)s.tar.gz",
                        a=args.architecture,
                        sh=dep["hash"][0:2],
                        h=dep["hash"],
                        p=dep["package"],
                        v=dep["version"],
                        r=dep["revision"])
        err = execute(format("cd %(workDir)s &&"
                             "mkdir -p %(target)s &&"
                             "ln -sfn %(source)s %(target)s",
                             workDir = args.workDir,
                             target=target,
                             source=source))
      if args.writeStore:
        cmd = format("cd %(w)s && "
                     "rsync -avR %(o)s --ignore-existing %(t)s/  %(rs)s/",
                     w=args.workDir,
                     rs=args.writeStore,
                     o=rsyncOptions,
                     t=target)
        execute(cmd)
      buildOrder.pop(0)
      packageIterations = 0
      continue

    debug("Looking for cached tarball in %s" % spec["tarballHashDir"])
    # FIXME: I should get the tarballHashDir updated with server at this point.
    #        It does not really matter that the symlinks are ok at this point
    #        as I only used the tarballs as reusable binary blobs.
    spec["cachedTarball"] = (glob("%s/*" % spec["tarballHashDir"]) or [""])[0]
    debug(spec["cachedTarball"] and
          "Found tarball in %s" % spec["cachedTarball"] or
          "No cache tarballs found")

    # FIXME: Why doing it here? This should really be done before anything else.
    updateReferenceRepos(args.referenceSources, p, spec)

    # Generate the part which sources the environment for all the dependencies.
    # Notice that we guarantee that a dependency is always sourced before the
    # parts depending on it, but we do not guaranteed anything for the order in
    # which unrelated components are activated.
    dependencies = ""
    dependenciesInit = ""
    for dep in spec.get("requires", []):
      depSpec = specs[dep]
      depInfo = {
        "architecture": args.architecture,
        "package": dep,
        "version": depSpec["version"],
        "revision": depSpec["revision"],
        "bigpackage": dep.upper().replace("-", "_")
      }
      dependencies += format("source \"$WORK_DIR/%(architecture)s/%(package)s/%(version)s-%(revision)s/etc/profile.d/init.sh\"\n",
                             **depInfo)
      dependenciesInit += format('echo source \${WORK_DIR}/%(architecture)s/%(package)s/%(version)s-%(revision)s/etc/profile.d/init.sh >> \"$INSTALLROOT/etc/profile.d/init.sh\"\n',
                             **depInfo)
    # Generate the part which creates the environment for the package.
    # This can be either variable set via the "env" keyword in the metadata
    # or paths which get appended via the "append_path" one.
    # By default we append LD_LIBRARY_PATH, PATH and DYLD_LIBRARY_PATH
    # FIXME: do not append variables for Mac on Linux.
    environment = ""
    if not isinstance(spec.get("env", {}), dict):
      print "Tag `env' in %s should be a dict." % p
      sys.exit(1)

    for key,value in spec.get("env", {}).iteritems():
      environment += format("echo 'export %(key)s=%(value)s' >> $INSTALLROOT/etc/profile.d/init.sh\n",
                            key=key,
                            value=value)
    basePath = "%s_ROOT" % p.upper().replace("-", "_")

    pathDict = spec.get("append_path", {})
    if not isinstance(pathDict, dict):
      print "Tag `append_path' in %s should be a dict." % p
      sys.exit(1)
    for pathName,pathVal in pathDict.iteritems():
      pathVal = isinstance(pathVal, list) and pathVal or [ pathVal ]
      environment += format("echo 'export %(key)s=%(value)s:$%(key)s' >> \"$INSTALLROOT/etc/profile.d/init.sh\"\n",
                            key=pathName,
                            value=":".join(pathVal))

    # Same thing, but prepending the results so that they win against system ones.
    defaultPrependPaths = { "LD_LIBRARY_PATH": "$%s/lib" % basePath,
                            "DYLD_LIBRARY_PATH": "$%s/lib" % basePath,
                            "PATH": "$%s/bin" % basePath }
    pathDict = spec.get("prepend_path", {})
    if not isinstance(pathDict, dict):
      print "Tag `prepend_path' in %s should be a dict." % p
      sys.exit(1)
    for pathName,pathVal in pathDict.iteritems():
      pathDict[pathName] = isinstance(pathVal, list) and pathVal or [ pathVal ]
    for pathName,pathVal in defaultPrependPaths.iteritems():
      pathDict[pathName] = [ pathVal ] + pathDict.get(pathName, [])
    for pathName,pathVal in pathDict.iteritems():
      environment += format("echo 'export %(key)s=%(value)s:$%(key)s' >> \"$INSTALLROOT/etc/profile.d/init.sh\"\n",
                            key=pathName,
                            value=":".join(pathVal))

    # The actual build script.
    referenceStatement = ""
    if "reference" in spec:
      referenceStatement = "export GIT_REFERENCE=%s" % spec["reference"]

    debug(spec)

    fp = open(dirname(realpath(__file__))+'/build_template.sh', 'r')
    cmd_raw = fp.read()
    fp.close()

    source = spec.get("source", "")
    # Shortend the commit hash in case it's a real commit hash and not simply
    # the tag.
    commit_hash = spec["commit_hash"]
    if spec["tag"] != spec["commit_hash"]:
      commit_hash = spec["commit_hash"][0:10]
    cmd = format(cmd_raw,
                 dependencies=dependencies,
                 dependenciesInit=dependenciesInit,
                 environment=environment,
                 workDir=workDir,
                 configDir=abspath(args.configDir),
                 pkgname=spec["package"],
                 hash=spec["hash"],
                 incremental_hash=spec.get("incremental_hash", ""),
                 incremental_recipe=spec.get("incremental_recipe", ":"),
                 version=spec["version"],
                 revision=spec["revision"],
                 architecture=args.architecture,
                 jobs=args.jobs,
                 source=source,
                 write_repo=spec.get("write_repo", source),
                 tag=spec["tag"],
                 commit_hash=commit_hash,
                 referenceStatement=referenceStatement,
                 cachedTarball=spec["cachedTarball"],
                 gzip=gzip())
    scriptName = "%s/SPECS/%s/%s/%s-%s/build.sh" % (workDir,
                                                    args.architecture,
                                                    spec["package"],
                                                    spec["version"],
                                                    spec["revision"])
    err, out = getstatusoutput("mkdir -p %s" % dirname(scriptName))
    script = open(scriptName, "w")
    script.write(cmd)
    script.flush()
    script.close()

    print "Building %s@%s" % (spec["package"], spec["version"])
    err = execute("/bin/bash -e -x %s 2>&1" % scriptName)
    if err:
      print "Error while executing %s" % scriptName
      sys.exit(1)
    if args.writeStore:
      tarballNameWithRev = format("%(package)s-%(version)s-%(revision)s.%(architecture)s.tar.gz",
                                  architecture=args.architecture,
                                  **spec)
      cmd = format("cd %(workdir)s && "
                   "rsync -avR %(rsyncOptions)s --ignore-existing %(storePath)s/%(tarballNameWithRev)s  %(remoteStore)s/ &&"
                   "rsync -avR %(rsyncOptions)s --ignore-existing %(linksPath)s/%(tarballNameWithRev)s  %(remoteStore)s/",
                   workdir=args.workDir,
                   remoteStore=args.remoteStore,
                   rsyncOptions=rsyncOptions,
                   storePath=spec["storePath"],
                   linksPath=spec["linksPath"],
                   tarballNameWithRev=tarballNameWithRev)
      err = execute(cmd)
      dieOnError(err, "Unable to upload tarball.")
